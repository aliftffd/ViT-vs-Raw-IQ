{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7acd731b-001e-4c34-a1a8-5675d81b8ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from torchinfo import summary\n",
    "from typing import Tuple, Optional\n",
    "import torch.utils.checkpoint as checkpoint\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm, trange\n",
    "import seaborn as sns\n",
    "import gc\n",
    "import time \n",
    "import seaborn \n",
    "from IPython.display import display, HTML\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import time\n",
    "import warnings\n",
    "from collections import deque\n",
    "import psutil\n",
    "import os\n",
    "import math\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "553632c7-c4a3-450c-9b8a-c304c0c98b7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Libraries imported successfully\n",
      "PyTorch version: 2.8.0+cu129\n",
      "CUDA available: True\n",
      "CUDA device: NVIDIA GeForce RTX 3050 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "# Set style for better visualizations\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "33208912-b22f-4381-a8a2-e155f7f876c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_PATH = \"/home/lipplopp/research/AMC_Repository/dataset/GOLD_XYZ_OSC.0001_1024.hdf5\"\n",
    "JSON_PATH = '/home/lipplopp/research/AMC_Repository/dataset/classes-fixed.json' \n",
    "BATCH_SIZE = 64  # Adjust based on your GPU memory\n",
    "NUM_WORKERS = 4  # Adjust based on your CPU cores\n",
    "patience = 10\n",
    "TARGET_MODULATIONS = [\n",
    "                      #'OOK',\n",
    "                      # '4ASK',\n",
    "                      '8ASK',\n",
    "                      'BPSK', \n",
    "                      #'QPSK',\n",
    "                      '8PSK', \n",
    "                      '16QAM',\n",
    "                      '64QAM', \n",
    "                      #'OQPSK'\n",
    "                     ]\n",
    "NUM_CLASSES = len(TARGET_MODULATIONS)\n",
    "NUM_EPOCHS = 100\n",
    "SUBSAMPLE_TRAIN_RATIO = 0.2  # Use 20% of training data (set to 1.0 for full data)\n",
    "TRAIN_SIZE = 0.7\n",
    "VALID_SIZE = 0.2\n",
    "TEST_SIZE = 0.1\n",
    "SPLIT_SEED = 48\n",
    "NORM_SEED = 49\n",
    "CHUNK_SIZE = 10000 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8d278651-c19d-455e-b8b9-d8496b0899c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Configuration loaded\n",
      "üìä Split ratio: Train=0.7, Valid=0.2, Test=0.1\n",
      "üíæ Batch size: 64\n",
      "üñ•Ô∏è Device: cuda\n",
      "üì¶ Chunk size for processing: 10000\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"üîß Configuration loaded\")\n",
    "print(f\"üìä Split ratio: Train={TRAIN_SIZE}, Valid={VALID_SIZE}, Test={TEST_SIZE}\")\n",
    "print(f\"üíæ Batch size: {BATCH_SIZE}\")\n",
    "print(f\"üñ•Ô∏è Device: {device}\")\n",
    "print(f\"üì¶ Chunk size for processing: {CHUNK_SIZE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "11720cee-ac04-4799-bb98-5d78d74f246f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset_metadata(file_path: str, json_path: str) -> Tuple[np.ndarray, np.ndarray, List[str], int]:\n",
    "    \"\"\"\n",
    "    Load only metadata and labels from HDF5 file (memory efficient).\n",
    "    \n",
    "    Returns:\n",
    "        (Y_strings, Z_data, available_modulations, total_samples)\n",
    "    \"\"\"\n",
    "    print(\"üìÇ Loading dataset metadata (memory efficient)...\")\n",
    "    \n",
    "    with h5py.File(file_path, 'r') as hdf5_file:\n",
    "        # Get dataset shape without loading data\n",
    "        total_samples = hdf5_file['X'].shape[0]\n",
    "        signal_length = hdf5_file['X'].shape[1]\n",
    "        num_channels = hdf5_file['X'].shape[2]\n",
    "        \n",
    "        print(f\"üìä Dataset shape: ({total_samples:,} √ó {signal_length} √ó {num_channels})\")\n",
    "        \n",
    "        # Load only labels (much smaller than signal data)\n",
    "        Y_int = np.argmax(hdf5_file['Y'][:], axis=1)\n",
    "        Z_data = hdf5_file['Z'][:, 0]\n",
    "        \n",
    "        # Load modulation classes\n",
    "        with open(json_path, 'r') as f:\n",
    "            modulation_classes = json.load(f)\n",
    "        \n",
    "        # Convert integer labels to string labels\n",
    "        Y_strings = np.array([modulation_classes[i] for i in Y_int])\n",
    "        \n",
    "        # Get available modulations\n",
    "        available_modulations = list(np.unique(Y_strings))\n",
    "        \n",
    "        print(f\"‚úÖ Metadata loaded: {total_samples:,} samples\")\n",
    "        print(f\"üì° Available modulations: {len(available_modulations)}\")\n",
    "        print(f\"üìä SNR range: {np.min(Z_data):.1f} to {np.max(Z_data):.1f} dB\")\n",
    "        \n",
    "        # Memory usage estimate\n",
    "        data_size_gb = (total_samples * signal_length * num_channels * 4) / (1024**3)\n",
    "        print(f\"üíæ Full dataset size: ~{data_size_gb:.2f} GB\")\n",
    "        \n",
    "    return Y_strings, Z_data, available_modulations, total_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6e1a8f87-4fe2-48b5-9b35-53b77a7e5a4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Memory-efficient functions defined\n"
     ]
    }
   ],
   "source": [
    "def stratified_dataset_split_memory_efficient(\n",
    "    modulations: np.ndarray, \n",
    "    snrs: np.ndarray, \n",
    "    target_modulations: List[str],\n",
    "    train_size: float = 0.7, \n",
    "    valid_size: float = 0.2, \n",
    "    test_size: float = 0.1,\n",
    "    seed: int = 48, \n",
    "    subsample_train_ratio: Optional[float] = None) -> Tuple[Dict, Dict]:\n",
    "    \"\"\"\n",
    "    Memory-efficient stratified split (doesn't require loading X data).\n",
    "    \n",
    "    Args:\n",
    "        modulations: Modulation labels (Y)\n",
    "        snrs: SNR labels (Z)\n",
    "        target_modulations: Modulations to include\n",
    "        train_size, valid_size, test_size: Split proportions\n",
    "        seed: Random seed\n",
    "        subsample_train_ratio: Optional subsampling of training data\n",
    "    \n",
    "    Returns:\n",
    "        (splits_dict, label_map)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Input validation\n",
    "    if not np.isclose(train_size + valid_size + test_size, 1.0, atol=1e-6):\n",
    "        raise ValueError(f\"Split sizes must sum to 1.0\")\n",
    "    \n",
    "    if len(target_modulations) == 0:\n",
    "        raise ValueError(\"target_modulations cannot be empty\")\n",
    "    \n",
    "    print(\"üîÑ Performing stratified split...\")\n",
    "    \n",
    "    # Filter to include only target modulations\n",
    "    target_mask = np.isin(modulations, target_modulations)\n",
    "    target_indices = np.where(target_mask)[0]\n",
    "    \n",
    "    if len(target_indices) == 0:\n",
    "        raise ValueError(\"No samples found for target modulations\")\n",
    "    \n",
    "    print(f\"üìä Found {len(target_indices):,} samples for target modulations\")\n",
    "    \n",
    "    filtered_modulations = modulations[target_indices]\n",
    "    filtered_snrs = snrs[target_indices]\n",
    "    \n",
    "    # Create stratification key\n",
    "    stratify_key = [f\"{mod}_{snr}\" for mod, snr in zip(filtered_modulations, filtered_snrs)]\n",
    "    \n",
    "    # Check sample distribution\n",
    "    unique_keys, key_counts = np.unique(stratify_key, return_counts=True)\n",
    "    min_samples_per_key = np.min(key_counts)\n",
    "    \n",
    "    if min_samples_per_key < 2:\n",
    "        warnings.warn(f\"‚ö†Ô∏è Some modulation-SNR combinations have only {min_samples_per_key} samples.\")\n",
    "    \n",
    "    # Set random seed\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # First split: separate test set\n",
    "    train_val_indices, test_indices = train_test_split(\n",
    "        target_indices, \n",
    "        test_size=test_size, \n",
    "        random_state=seed, \n",
    "        stratify=stratify_key\n",
    "    )\n",
    "    \n",
    "    # Second split: separate train and validation\n",
    "    remaining_modulations = modulations[train_val_indices]\n",
    "    remaining_snrs = snrs[train_val_indices]\n",
    "    remaining_stratify_key = [f\"{mod}_{snr}\" for mod, snr in zip(remaining_modulations, remaining_snrs)]\n",
    "    \n",
    "    relative_valid_size = valid_size / (1 - test_size)\n",
    "    \n",
    "    train_indices, valid_indices = train_test_split(\n",
    "        train_val_indices,\n",
    "        test_size=relative_valid_size,\n",
    "        random_state=seed,\n",
    "        stratify=remaining_stratify_key\n",
    "    )\n",
    "    \n",
    "    # Apply subsampling if requested\n",
    "    if subsample_train_ratio is not None and subsample_train_ratio < 1.0:\n",
    "        n_keep = int(len(train_indices) * subsample_train_ratio)\n",
    "        if n_keep == 0:\n",
    "            raise ValueError(\"Subsampling ratio too small\")\n",
    "        \n",
    "        np.random.seed(seed)\n",
    "        train_indices = np.random.choice(train_indices, n_keep, replace=False)\n",
    "        print(f\"üìâ Subsampled training data to {n_keep:,} samples ({subsample_train_ratio:.1%})\")\n",
    "    \n",
    "    # Create label mapping\n",
    "    label_map = {name: i for i, name in enumerate(target_modulations)}\n",
    "    \n",
    "    # Print statistics\n",
    "    print(f\"\\n‚úÖ Dataset split completed:\")\n",
    "    print(f\"  üìö Train: {len(train_indices):,} samples\")\n",
    "    print(f\"  üîç Valid: {len(valid_indices):,} samples\") \n",
    "    print(f\"  üß™ Test: {len(test_indices):,} samples\")\n",
    "    \n",
    "    splits = {\n",
    "        'train': train_indices,\n",
    "        'valid': valid_indices,\n",
    "        'test': test_indices\n",
    "    }\n",
    "    \n",
    "    return splits, label_map\n",
    "\n",
    "print(\"‚úÖ Memory-efficient functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ab34b1e-fc23-4f08-820d-0fe3f7686e75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Splitting function defined\n"
     ]
    }
   ],
   "source": [
    "\"\"\" NOt evicient for memory\"\"\"\n",
    "# def stratified_dataset_split(data: np.ndarray, \n",
    "#                            modulations: np.ndarray, \n",
    "#                            snrs: np.ndarray, \n",
    "#                            target_modulations: List[str],\n",
    "#                            train_size: float = 0.7, \n",
    "#                            valid_size: float = 0.2, \n",
    "#                            test_size: float = 0.1,\n",
    "#                            seed: int = 48, \n",
    "#                            subsample_train_ratio: Optional[float] = None) -> Tuple[Dict, Dict]:\n",
    "#     # Input validation\n",
    "#     if not np.isclose(train_size + valid_size + test_size, 1.0, atol=1e-6):\n",
    "#         raise ValueError(f\"Split sizes must sum to 1.0, got {train_size + valid_size + test_size}\")\n",
    "    \n",
    "#     if len(target_modulations) == 0:\n",
    "#         raise ValueError(\"target_modulations cannot be empty\")\n",
    "    \n",
    "#     if subsample_train_ratio is not None and (subsample_train_ratio <= 0 or subsample_train_ratio > 1.0):\n",
    "#         raise ValueError(\"subsample_train_ratio must be between 0 and 1\")\n",
    "    \n",
    "#     print(\"üîÑ Performing stratified split on dataset...\")\n",
    "    \n",
    "#     # Filter data to include only target modulations\n",
    "#     target_mask = np.isin(modulations, target_modulations)\n",
    "#     target_indices = np.where(target_mask)[0]\n",
    "    \n",
    "#     if len(target_indices) == 0:\n",
    "#         raise ValueError(\"No samples found for target modulations\")\n",
    "    \n",
    "#     filtered_modulations = modulations[target_indices]\n",
    "#     filtered_snrs = snrs[target_indices]\n",
    "    \n",
    "#     # Create stratification key combining modulation and SNR\n",
    "#     stratify_key = [f\"{mod}_{snr}\" for mod, snr in zip(filtered_modulations, filtered_snrs)]\n",
    "    \n",
    "#     # Check if we have enough samples for stratification\n",
    "#     unique_keys, key_counts = np.unique(stratify_key, return_counts=True)\n",
    "#     min_samples_per_key = np.min(key_counts)\n",
    "    \n",
    "#     if min_samples_per_key < 2:\n",
    "#         warnings.warn(f\"‚ö†Ô∏è Some modulation-SNR combinations have only {min_samples_per_key} samples.\")\n",
    "    \n",
    "#     # Set random seed for reproducibility\n",
    "#     np.random.seed(seed)\n",
    "    \n",
    "#     # First split: separate test set\n",
    "#     train_val_indices, test_indices = train_test_split(\n",
    "#         target_indices, \n",
    "#         test_size=test_size, \n",
    "#         random_state=seed, \n",
    "#         stratify=stratify_key\n",
    "#     )\n",
    "    \n",
    "#     # Second split: separate train and validation\n",
    "#     remaining_modulations = modulations[train_val_indices]\n",
    "#     remaining_snrs = snrs[train_val_indices]\n",
    "#     remaining_stratify_key = [f\"{mod}_{snr}\" for mod, snr in zip(remaining_modulations, remaining_snrs)]\n",
    "    \n",
    "#     relative_valid_size = valid_size / (1 - test_size)\n",
    "    \n",
    "#     train_indices, valid_indices = train_test_split(\n",
    "#         train_val_indices,\n",
    "#         test_size=relative_valid_size,\n",
    "#         random_state=seed,\n",
    "#         stratify=remaining_stratify_key\n",
    "#     )\n",
    "    \n",
    "#     # Apply training data subsampling if requested\n",
    "#     if subsample_train_ratio is not None and subsample_train_ratio < 1.0:\n",
    "#         n_keep = int(len(train_indices) * subsample_train_ratio)\n",
    "#         if n_keep == 0:\n",
    "#             raise ValueError(\"Subsampling ratio too small\")\n",
    "        \n",
    "#         np.random.seed(seed)\n",
    "#         train_indices = np.random.choice(train_indices, n_keep, replace=False)\n",
    "#         print(f\"üìâ Subsampled training data to {n_keep} samples ({subsample_train_ratio:.1%})\")\n",
    "    \n",
    "#     # Create label mapping\n",
    "#     label_map = {name: i for i, name in enumerate(target_modulations)}\n",
    "    \n",
    "#     # Print split statistics\n",
    "#     print(f\"\\n‚úÖ Dataset split completed:\")\n",
    "#     print(f\"  üìö Train: {len(train_indices)} samples\")\n",
    "#     print(f\"  üîç Valid: {len(valid_indices)} samples\") \n",
    "#     print(f\"  üß™ Test: {len(test_indices)} samples\")\n",
    "    \n",
    "#     splits = {\n",
    "#         'train': train_indices,\n",
    "#         'valid': valid_indices,\n",
    "#         'test': test_indices\n",
    "#     }\n",
    "    \n",
    "#     return splits, label_map\n",
    "\n",
    "# print(\"‚úÖ Splitting function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1155b591-2e45-4bc0-a80c-12761f2e8520",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Memory-efficient dataset class defined\n"
     ]
    }
   ],
   "source": [
    "class DualStreamRadioMLDataset(Dataset):\n",
    "    \"\"\"Memory-efficient dataset class that reads from HDF5 on-demand.\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 file_path: str, \n",
    "                 json_path: str, \n",
    "                 target_modulations: List[str], \n",
    "                 mode: str, \n",
    "                 indices: np.ndarray, \n",
    "                 label_map: Dict[str, int], \n",
    "                 normalization_stats: Optional[Dict] = None, \n",
    "                 seed: int = 49):\n",
    "        \"\"\"Initialize the dataset.\"\"\"\n",
    "        super(DualStreamRadioMLDataset, self).__init__()\n",
    "\n",
    "        # Validate inputs\n",
    "        if mode not in ['train', 'valid', 'test']:\n",
    "            raise ValueError(f\"mode must be 'train', 'valid', or 'test'\")\n",
    "        \n",
    "        if len(indices) == 0:\n",
    "            raise ValueError(\"indices cannot be empty\")\n",
    "        \n",
    "        # Store parameters\n",
    "        self.file_path = file_path\n",
    "        self.json_path = json_path\n",
    "        self.target_modulations = target_modulations\n",
    "        self.mode = mode\n",
    "        self.indices = np.array(indices, dtype=int)\n",
    "        self.label_map = label_map\n",
    "        self.seed = seed\n",
    "\n",
    "        # Open HDF5 file in read mode (keep it open for efficiency)\n",
    "        self.hdf5_file = h5py.File(self.file_path, 'r')\n",
    "        self.X_h5 = self.hdf5_file['X']  # Reference, not loaded into memory\n",
    "        \n",
    "        # Load only labels (small memory footprint)\n",
    "        self.Y_int = np.argmax(self.hdf5_file['Y'][:], axis=1)\n",
    "        self.Z = self.hdf5_file['Z'][:, 0]\n",
    "\n",
    "        # Load modulation classes\n",
    "        with open(self.json_path, 'r') as f:\n",
    "            self.modulation_classes = json.load(f)\n",
    "\n",
    "        self.Y_strings = np.array([self.modulation_classes[i] for i in self.Y_int])\n",
    "\n",
    "        # Validate signal dimensions\n",
    "        signal_length = self.X_h5.shape[1]\n",
    "        if signal_length != 1024:\n",
    "            raise ValueError(f\"Expected signal length 1024, got {signal_length}\")\n",
    "        self.H, self.W = 32, 32\n",
    "\n",
    "        # Handle normalization statistics\n",
    "        if mode == 'train':\n",
    "            if normalization_stats is None:\n",
    "                print(f\"üìä Calculating normalization stats for {mode} mode...\")\n",
    "                self.norm_stats = self._calculate_normalization_stats()\n",
    "                print(f\"‚úÖ Stats calculated\")\n",
    "            else:\n",
    "                self.norm_stats = normalization_stats\n",
    "        else:\n",
    "            if normalization_stats is None:\n",
    "                raise ValueError(f\"normalization_stats required for '{mode}' mode\")\n",
    "            self.norm_stats = normalization_stats\n",
    "\n",
    "        print(f\"‚úÖ {mode.capitalize()} dataset: {len(self.indices):,} samples (memory-efficient mode)\")\n",
    "\n",
    "    def _calculate_normalization_stats(self) -> Dict[str, float]:\n",
    "        \"\"\"Calculate normalization statistics using chunked processing.\"\"\"\n",
    "        num_samples = min(5000, len(self.indices))\n",
    "        \n",
    "        np.random.seed(self.seed)\n",
    "        sample_indices = np.random.choice(self.indices, num_samples, replace=False)\n",
    "        sorted_indices = np.sort(sample_indices)\n",
    "        \n",
    "        # Process in chunks to avoid memory issues\n",
    "        chunk_size = min(500, num_samples)  # Smaller chunks for stats calculation\n",
    "        \n",
    "        i_vals = []\n",
    "        q_vals = []\n",
    "        amp_vals = []\n",
    "        \n",
    "        print(f\"  Processing {num_samples} samples in chunks of {chunk_size}...\")\n",
    "        \n",
    "        for i in range(0, len(sorted_indices), chunk_size):\n",
    "            chunk_indices = sorted_indices[i:i+chunk_size]\n",
    "            chunk_data = self.X_h5[chunk_indices, ...]\n",
    "            \n",
    "            # Convert to tensor\n",
    "            chunk_tensor = torch.from_numpy(chunk_data).float()\n",
    "            \n",
    "            # Extract I/Q values\n",
    "            i_chunk = chunk_tensor[:, :, 0].flatten()\n",
    "            q_chunk = chunk_tensor[:, :, 1].flatten()\n",
    "            amp_chunk = torch.sqrt(i_chunk**2 + q_chunk**2)\n",
    "            \n",
    "            i_vals.append(i_chunk)\n",
    "            q_vals.append(q_chunk)\n",
    "            amp_vals.append(amp_chunk)\n",
    "            \n",
    "            # Clear chunk data to free memory\n",
    "            del chunk_data, chunk_tensor\n",
    "        \n",
    "        # Concatenate all chunks\n",
    "        i_all = torch.cat(i_vals)\n",
    "        q_all = torch.cat(q_vals)\n",
    "        amp_all = torch.cat(amp_vals)\n",
    "        \n",
    "        stats = {\n",
    "            'i_mean': i_all.mean().item(),\n",
    "            'i_std': i_all.std().item(),\n",
    "            'q_mean': q_all.mean().item(), \n",
    "            'q_std': q_all.std().item(),\n",
    "            'amp_max': amp_all.max().item()\n",
    "        }\n",
    "        \n",
    "        # Validate statistics\n",
    "        if stats['i_std'] == 0 or stats['q_std'] == 0:\n",
    "            warnings.warn(\"‚ö†Ô∏è Std dev is 0, adding epsilon\")\n",
    "            stats['i_std'] = max(stats['i_std'], 1e-8)\n",
    "            stats['q_std'] = max(stats['q_std'], 1e-8)\n",
    "        \n",
    "        # Clean up\n",
    "        del i_vals, q_vals, amp_vals, i_all, q_all, amp_all\n",
    "        gc.collect()\n",
    "        \n",
    "        return stats\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.indices)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, int, float]:\n",
    "        \"\"\"Get a single sample (loaded on-demand from HDF5).\"\"\"\n",
    "        if idx >= len(self.indices):\n",
    "            raise IndexError(f\"Index {idx} out of range\")\n",
    "        \n",
    "        true_index = int(self.indices[idx])\n",
    "        \n",
    "        # Load only this single sample from disk\n",
    "        x_raw = self.X_h5[true_index]\n",
    "        y_string = self.Y_strings[true_index]\n",
    "        z = float(self.Z[true_index])\n",
    "        \n",
    "        if y_string not in self.label_map:\n",
    "            raise ValueError(f\"Modulation '{y_string}' not found\")\n",
    "        y = self.label_map[y_string]\n",
    "\n",
    "        # Convert to tensor and normalize\n",
    "        iq_sequence = torch.from_numpy(x_raw.copy()).float()  # .copy() ensures contiguous memory\n",
    "        iq_sequence[:, 0] = (iq_sequence[:, 0] - self.norm_stats['i_mean']) / self.norm_stats['i_std']\n",
    "        iq_sequence[:, 1] = (iq_sequence[:, 1] - self.norm_stats['q_mean']) / self.norm_stats['q_std']\n",
    "\n",
    "        # Calculate amplitude and phase\n",
    "        i_signal = iq_sequence[:, 0]\n",
    "        q_signal = iq_sequence[:, 1]\n",
    "        amplitude = torch.sqrt(i_signal**2 + q_signal**2)\n",
    "        phase = torch.atan2(q_signal, i_signal)\n",
    "\n",
    "        # Reshape to 2D\n",
    "        amplitude_2d = amplitude.view(1, self.H, self.W)\n",
    "        phase_2d = phase.view(1, self.H, self.W)\n",
    "\n",
    "        # Normalize\n",
    "        amplitude_2d = amplitude_2d / self.norm_stats['amp_max']\n",
    "        phase_2d = phase_2d / math.pi\n",
    "        \n",
    "        return amplitude_2d, phase_2d, iq_sequence, y, z\n",
    "\n",
    "    def get_normalization_stats(self) -> Dict[str, float]:\n",
    "        return self.norm_stats.copy()\n",
    "\n",
    "    def get_class_distribution(self) -> Dict[str, int]:\n",
    "        y_strings_subset = self.Y_strings[self.indices]\n",
    "        unique, counts = np.unique(y_strings_subset, return_counts=True)\n",
    "        return dict(zip(unique, counts))\n",
    "\n",
    "    def get_snr_distribution(self) -> Dict[float, int]:\n",
    "        z_subset = self.Z[self.indices]\n",
    "        unique, counts = np.unique(z_subset, return_counts=True)\n",
    "        return dict(zip(unique, counts))\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\"Important: Close the HDF5 file when done.\"\"\"\n",
    "        if hasattr(self, 'hdf5_file') and self.hdf5_file is not None:\n",
    "            try:\n",
    "                self.hdf5_file.close()\n",
    "                print(f\"üîí {self.mode.capitalize()} dataset: HDF5 file closed\")\n",
    "            except:\n",
    "                pass\n",
    "            finally:\n",
    "                self.hdf5_file = None\n",
    "\n",
    "    def __del__(self):\n",
    "        self.close()\n",
    "\n",
    "print(\"‚úÖ Memory-efficient dataset class defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d3f771db-a400-4748-941b-8b2e611bcab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Dataset class defined\n"
     ]
    }
   ],
   "source": [
    "\"\"Not efficient\"\"\"\n",
    "# class DualStreamRadioMLDataset(Dataset):\n",
    "#     \"\"\"Dataset class for dual-stream radio ML data (Version 3).\"\"\"\n",
    "    \n",
    "#     def __init__(self, \n",
    "#                  file_path: str, \n",
    "#                  json_path: str, \n",
    "#                  target_modulations: List[str], \n",
    "#                  mode: str, \n",
    "#                  indices: np.ndarray, \n",
    "#                  label_map: Dict[str, int], \n",
    "#                  normalization_stats: Optional[Dict] = None, \n",
    "#                  seed: int = 49):\n",
    "#         \"\"\"Initialize the dataset.\"\"\"\n",
    "#         super(DualStreamRadioMLDataset, self).__init__()\n",
    "\n",
    "#         # Validate inputs\n",
    "#         if mode not in ['train', 'valid', 'test']:\n",
    "#             raise ValueError(f\"mode must be 'train', 'valid', or 'test', got '{mode}'\")\n",
    "        \n",
    "#         if len(indices) == 0:\n",
    "#             raise ValueError(\"indices cannot be empty\")\n",
    "        \n",
    "#         # Store parameters\n",
    "#         self.file_path = file_path\n",
    "#         self.json_path = json_path\n",
    "#         self.target_modulations = target_modulations\n",
    "#         self.mode = mode\n",
    "#         self.indices = np.array(indices, dtype=int)\n",
    "#         self.label_map = label_map\n",
    "#         self.seed = seed\n",
    "\n",
    "#         # Open HDF5 file and load metadata\n",
    "#         self.hdf5_file = h5py.File(self.file_path, 'r')\n",
    "#         self.X_h5 = self.hdf5_file['X']\n",
    "#         self.Y_int = np.argmax(self.hdf5_file['Y'][:], axis=1)\n",
    "#         self.Z = self.hdf5_file['Z'][:, 0]\n",
    "\n",
    "#         # Load modulation classes\n",
    "#         with open(self.json_path, 'r') as f:\n",
    "#             self.modulation_classes = json.load(f)\n",
    "\n",
    "#         self.Y_strings = np.array([self.modulation_classes[i] for i in self.Y_int])\n",
    "\n",
    "#         # Validate signal dimensions\n",
    "#         signal_length = self.X_h5.shape[1]\n",
    "#         if signal_length != 1024:\n",
    "#             raise ValueError(f\"Expected signal length 1024, got {signal_length}\")\n",
    "#         self.H, self.W = 32, 32\n",
    "\n",
    "#         # Handle normalization statistics\n",
    "#         if mode == 'train':\n",
    "#             if normalization_stats is None:\n",
    "#                 print(f\"üìä Calculating normalization stats for {mode} mode...\")\n",
    "#                 self.norm_stats = self._calculate_normalization_stats()\n",
    "#                 print(f\"‚úÖ Stats: {self.norm_stats}\")\n",
    "#             else:\n",
    "#                 print(f\"üìà Using provided normalization stats for {mode} mode\")\n",
    "#                 self.norm_stats = normalization_stats\n",
    "#         else:\n",
    "#             if normalization_stats is None:\n",
    "#                 raise ValueError(f\"normalization_stats required for '{mode}' mode\")\n",
    "#             print(f\"üìà Using provided normalization stats for {mode} mode\")\n",
    "#             self.norm_stats = normalization_stats\n",
    "\n",
    "#         print(f\"‚úÖ {mode.capitalize()} dataset: {len(self.indices)} samples\")\n",
    "\n",
    "#     def _calculate_normalization_stats(self) -> Dict[str, float]:\n",
    "#         \"\"\"Calculate normalization statistics.\"\"\"\n",
    "#         num_samples = min(5000, len(self.indices))\n",
    "        \n",
    "#         np.random.seed(self.seed)\n",
    "#         sample_indices = np.random.choice(self.indices, num_samples, replace=False)\n",
    "#         sorted_sample_indices = np.sort(sample_indices)\n",
    "        \n",
    "#         sample_data = torch.from_numpy(self.X_h5[sorted_sample_indices, ...]).float()\n",
    "\n",
    "#         i_flat = sample_data[:, :, 0].flatten()\n",
    "#         q_flat = sample_data[:, :, 1].flatten()\n",
    "#         amplitude = torch.sqrt(i_flat**2 + q_flat**2)\n",
    "\n",
    "#         stats = {\n",
    "#             'i_mean': i_flat.mean().item(),\n",
    "#             'i_std': i_flat.std().item(),\n",
    "#             'q_mean': q_flat.mean().item(), \n",
    "#             'q_std': q_flat.std().item(),\n",
    "#             'amp_max': amplitude.max().item()\n",
    "#         }\n",
    "        \n",
    "#         if stats['i_std'] == 0 or stats['q_std'] == 0:\n",
    "#             warnings.warn(\"‚ö†Ô∏è Std dev is 0, adding epsilon\")\n",
    "#             stats['i_std'] = max(stats['i_std'], 1e-8)\n",
    "#             stats['q_std'] = max(stats['q_std'], 1e-8)\n",
    "        \n",
    "#         return stats\n",
    "\n",
    "#     def __len__(self) -> int:\n",
    "#         return len(self.indices)\n",
    "\n",
    "#     def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, int, float]:\n",
    "#         \"\"\"Get a single sample.\"\"\"\n",
    "#         if idx >= len(self.indices):\n",
    "#             raise IndexError(f\"Index {idx} out of range\")\n",
    "        \n",
    "#         true_index = int(self.indices[idx])\n",
    "        \n",
    "#         # Load raw data\n",
    "#         x_raw = self.X_h5[true_index]\n",
    "#         y_string = self.Y_strings[true_index]\n",
    "#         z = float(self.Z[true_index])\n",
    "        \n",
    "#         if y_string not in self.label_map:\n",
    "#             raise ValueError(f\"Modulation '{y_string}' not found\")\n",
    "#         y = self.label_map[y_string]\n",
    "\n",
    "#         # Normalize I/Q channels\n",
    "#         iq_sequence = torch.from_numpy(x_raw).float()\n",
    "#         iq_sequence[:, 0] = (iq_sequence[:, 0] - self.norm_stats['i_mean']) / self.norm_stats['i_std']\n",
    "#         iq_sequence[:, 1] = (iq_sequence[:, 1] - self.norm_stats['q_mean']) / self.norm_stats['q_std']\n",
    "\n",
    "#         # Calculate amplitude and phase\n",
    "#         i_signal = iq_sequence[:, 0]\n",
    "#         q_signal = iq_sequence[:, 1]\n",
    "#         amplitude = torch.sqrt(i_signal**2 + q_signal**2)\n",
    "#         phase = torch.atan2(q_signal, i_signal)\n",
    "\n",
    "#         # Reshape to 2D format (32x32)\n",
    "#         amplitude_2d = amplitude.view(1, self.H, self.W)\n",
    "#         phase_2d = phase.view(1, self.H, self.W)\n",
    "\n",
    "#         # Normalize\n",
    "#         amplitude_2d = amplitude_2d / self.norm_stats['amp_max']\n",
    "#         phase_2d = phase_2d / math.pi\n",
    "        \n",
    "#         return amplitude_2d, phase_2d, iq_sequence, y, z\n",
    "\n",
    "#     def get_normalization_stats(self) -> Dict[str, float]:\n",
    "#         return self.norm_stats.copy()\n",
    "\n",
    "#     def get_class_distribution(self) -> Dict[str, int]:\n",
    "#         y_strings_subset = self.Y_strings[self.indices]\n",
    "#         unique, counts = np.unique(y_strings_subset, return_counts=True)\n",
    "#         return dict(zip(unique, counts))\n",
    "\n",
    "#     def get_snr_distribution(self) -> Dict[float, int]:\n",
    "#         z_subset = self.Z[self.indices]\n",
    "#         unique, counts = np.unique(z_subset, return_counts=True)\n",
    "#         return dict(zip(unique, counts))\n",
    "\n",
    "#     def close(self):\n",
    "#         if hasattr(self, 'hdf5_file') and self.hdf5_file is not None:\n",
    "#             try:\n",
    "#                 self.hdf5_file.close()\n",
    "#                 print(f\"üîí {self.mode.capitalize()} dataset: HDF5 file closed\")\n",
    "#             except:\n",
    "#                 pass\n",
    "#             finally:\n",
    "#                 self.hdf5_file = None\n",
    "\n",
    "#     def __del__(self):\n",
    "#         self.close()\n",
    "\n",
    "# print(\"‚úÖ Dataset class defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cefc23e7-f512-4ba4-aa94-f50b52c3c799",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_data_distribution(datasets: Dict, label_map: Dict):\n",
    "    \"\"\"Visualize the distribution of data across splits.\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "    fig.suptitle('Dataset Distribution Analysis', fontsize=16)\n",
    "    \n",
    "    for idx, (split_name, dataset) in enumerate(datasets.items()):\n",
    "        # Class distribution\n",
    "        class_dist = dataset.get_class_distribution()\n",
    "        ax = axes[0, idx]\n",
    "        ax.bar(range(len(class_dist)), list(class_dist.values()))\n",
    "        ax.set_title(f'{split_name.capitalize()} - Class Distribution')\n",
    "        ax.set_xlabel('Modulation Type')\n",
    "        ax.set_ylabel('Sample Count')\n",
    "        ax.set_xticks(range(len(class_dist)))\n",
    "        ax.set_xticklabels(list(class_dist.keys()), rotation=45, ha='right')\n",
    "        \n",
    "        # SNR distribution\n",
    "        snr_dist = dataset.get_snr_distribution()\n",
    "        ax = axes[1, idx]\n",
    "        snrs = list(snr_dist.keys())\n",
    "        counts = list(snr_dist.values())\n",
    "        ax.bar(snrs, counts)\n",
    "        ax.set_title(f'{split_name.capitalize()} - SNR Distribution')\n",
    "        ax.set_xlabel('SNR (dB)')\n",
    "        ax.set_ylabel('Sample Count')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "22acbf14-c046-4dd6-92e7-9f1be18bbcff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Helper functions defined\n"
     ]
    }
   ],
   "source": [
    "def test_batch_loading(loader: DataLoader, name: str = \"Train\", show_plot: bool = True):\n",
    "    \"\"\"Test loading a batch and display statistics.\"\"\"\n",
    "    print(f\"\\nüß™ Testing {name} loader...\")\n",
    "    \n",
    "    for batch_idx, (amp, phase, iq, labels, snrs) in enumerate(loader):\n",
    "        print(f\"\\n  Batch {batch_idx + 1}:\")\n",
    "        print(f\"  ‚îú‚îÄ Amplitude shape: {amp.shape}\")\n",
    "        print(f\"  ‚îú‚îÄ Phase shape: {phase.shape}\")\n",
    "        print(f\"  ‚îú‚îÄ IQ sequence shape: {iq.shape}\")\n",
    "        print(f\"  ‚îú‚îÄ Labels shape: {labels.shape}\")\n",
    "        print(f\"  ‚îú‚îÄ SNRs shape: {snrs.shape}\")\n",
    "        print(f\"  ‚îú‚îÄ Label range: [{labels.min().item()}, {labels.max().item()}]\")\n",
    "        print(f\"  ‚îî‚îÄ SNR range: [{snrs.min().item():.1f}, {snrs.max().item():.1f}] dB\")\n",
    "        \n",
    "        if batch_idx == 0 and show_plot:\n",
    "            # Visualize first sample\n",
    "            fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "            \n",
    "            # Amplitude\n",
    "            axes[0].imshow(amp[0, 0].cpu().numpy(), cmap='hot')\n",
    "            axes[0].set_title(f'Amplitude (Label: {labels[0].item()})')\n",
    "            axes[0].axis('off')\n",
    "            \n",
    "            # Phase\n",
    "            axes[1].imshow(phase[0, 0].cpu().numpy(), cmap='hsv')\n",
    "            axes[1].set_title(f'Phase (SNR: {snrs[0].item():.1f} dB)')\n",
    "            axes[1].axis('off')\n",
    "            \n",
    "            # IQ time series\n",
    "            axes[2].plot(iq[0, :, 0].cpu().numpy(), label='I', alpha=0.7)\n",
    "            axes[2].plot(iq[0, :, 1].cpu().numpy(), label='Q', alpha=0.7)\n",
    "            axes[2].set_title('I/Q Signals')\n",
    "            axes[2].set_xlabel('Sample')\n",
    "            axes[2].set_ylabel('Amplitude')\n",
    "            axes[2].legend()\n",
    "            axes[2].grid(True, alpha=0.3)\n",
    "            \n",
    "            plt.suptitle(f'Sample from {name} Batch', fontsize=14)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            break\n",
    "\n",
    "print(\"‚úÖ Helper functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "79279d54-5a66-4afd-86e3-a5a7ab5d2e22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STEP 1: LOADING DATASET METADATA\n",
      "============================================================\n",
      "üìÇ Loading dataset metadata (memory efficient)...\n",
      "üìä Dataset shape: (2,555,904 √ó 1024 √ó 2)\n",
      "‚úÖ Metadata loaded: 2,555,904 samples\n",
      "üì° Available modulations: 24\n",
      "üìä SNR range: -20.0 to 30.0 dB\n",
      "üíæ Full dataset size: ~19.50 GB\n",
      "\n",
      "üì° Available modulations (24):\n",
      "   0. 128APSK\n",
      "   1. 128QAM\n",
      "   2. 16APSK\n",
      "   3. 16PSK\n",
      "   4. 16QAM\n",
      "   5. 256QAM\n",
      "   6. 32APSK\n",
      "   7. 32PSK\n",
      "   8. 32QAM\n",
      "   9. 4ASK\n",
      "  10. 64APSK\n",
      "  11. 64QAM\n",
      "  12. 8ASK\n",
      "  13. 8PSK\n",
      "  14. AM-DSB-SC\n",
      "  15. AM-DSB-WC\n",
      "  16. AM-SSB-SC\n",
      "  17. AM-SSB-WC\n",
      "  18. BPSK\n",
      "  19. FM\n",
      "  20. GMSK\n",
      "  21. OOK\n",
      "  22. OQPSK\n",
      "  23. QPSK\n",
      "\n",
      "üéØ Using 5 target modulations\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# Initialize tracking variables\n",
    "X_data = None\n",
    "Y_strings = None\n",
    "Z_data = None\n",
    "train_dataset = None\n",
    "valid_dataset = None\n",
    "test_dataset = None\n",
    "\n",
    "try:\n",
    "    # ============= Load Metadata Only =============\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"STEP 1: LOADING DATASET METADATA\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    Y_strings, Z_data, available_modulations, total_samples = load_dataset_metadata(\n",
    "        FILE_PATH, JSON_PATH\n",
    "    )\n",
    "    \n",
    "    # Display available modulations\n",
    "    print(f\"\\nüì° Available modulations ({len(available_modulations)}):\")\n",
    "    for i, mod in enumerate(available_modulations):\n",
    "        print(f\"  {i:2d}. {mod}\")\n",
    "    \n",
    "    # Set target modulations\n",
    "    target_modulations = TARGET_MODULATIONS\n",
    "    \n",
    "    print(f\"\\nüéØ Using {len(target_modulations)} target modulations\")\n",
    "    \n",
    "    # Clean up memory\n",
    "    gc.collect()\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"\\n‚ùå Error: File not found - {e}\")\n",
    "    print(\"Please update FILE_PATH and JSON_PATH in the configuration section.\")\n",
    "    raise\n",
    "except MemoryError as e:\n",
    "    print(f\"\\n‚ùå Memory Error: {e}\")\n",
    "    print(\"The dataset is too large. The memory-efficient version should handle this.\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7f87f412-c286-495c-a957-38765ab6dbec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Performing stratified split...\n",
      "üìä Found 532,480 samples for target modulations\n",
      "üìâ Subsampled training data to 74,547 samples (20.0%)\n",
      "\n",
      "‚úÖ Dataset split completed:\n",
      "  üìö Train: 74,547 samples\n",
      "  üîç Valid: 106,497 samples\n",
      "  üß™ Test: 53,248 samples\n",
      "\n",
      "üè∑Ô∏è Label mapping:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Modulation</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8ASK</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BPSK</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8PSK</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16QAM</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>64QAM</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Modulation  Label\n",
       "0       8ASK      0\n",
       "1       BPSK      1\n",
       "2       8PSK      2\n",
       "3      16QAM      3\n",
       "4      64QAM      4"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "splits, label_map = stratified_dataset_split_memory_efficient(\n",
    "    Y_strings, Z_data, target_modulations,\n",
    "    train_size=TRAIN_SIZE, \n",
    "    valid_size=VALID_SIZE, \n",
    "    test_size=TEST_SIZE,\n",
    "    seed=SPLIT_SEED,\n",
    "    subsample_train_ratio=SUBSAMPLE_TRAIN_RATIO\n",
    ")\n",
    "\n",
    "# Display label mapping\n",
    "print(\"\\nüè∑Ô∏è Label mapping:\")\n",
    "label_df = pd.DataFrame(list(label_map.items()), columns=['Modulation', 'Label'])\n",
    "display(label_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1ea45cbc-3f67-40a2-996c-c84e98f11257",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clean up - we don't need full Y_strings and Z_data anymore\n",
    "del Y_strings, Z_data\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4570718f-aa1a-4b57-ba86-fe82fceba617",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STEP 3: CREATING DATASETS (Memory Efficient)\n",
      "============================================================\n",
      "Creating training dataset...\n",
      "üìä Calculating normalization stats for train mode...\n",
      "  Processing 5000 samples in chunks of 500...\n",
      "‚úÖ Stats calculated\n",
      "‚úÖ Train dataset: 74,547 samples (memory-efficient mode)\n",
      "\n",
      "Creating validation dataset...\n",
      "‚úÖ Valid dataset: 106,497 samples (memory-efficient mode)\n",
      "\n",
      "Creating test dataset...\n",
      "‚úÖ Test dataset: 53,248 samples (memory-efficient mode)\n",
      "\n",
      "‚úÖ All datasets created successfully!\n"
     ]
    }
   ],
   "source": [
    "# ============= Create Datasets =============\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 3: CREATING DATASETS (Memory Efficient)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create train dataset\n",
    "print(\"Creating training dataset...\")\n",
    "train_dataset = DualStreamRadioMLDataset(\n",
    "    FILE_PATH, JSON_PATH, target_modulations,\n",
    "    mode='train', \n",
    "    indices=splits['train'], \n",
    "    label_map=label_map,\n",
    "    normalization_stats=None,  # Will be calculated\n",
    "    seed=NORM_SEED\n",
    ")\n",
    "\n",
    "# Get normalization stats\n",
    "train_norm_stats = train_dataset.get_normalization_stats()\n",
    "\n",
    "# Create validation dataset\n",
    "print(\"\\nCreating validation dataset...\")\n",
    "valid_dataset = DualStreamRadioMLDataset(\n",
    "    FILE_PATH, JSON_PATH, target_modulations,\n",
    "    mode='valid',\n",
    "    indices=splits['valid'],\n",
    "    label_map=label_map,\n",
    "    normalization_stats=train_norm_stats,\n",
    "    seed=NORM_SEED\n",
    ")\n",
    "\n",
    "# Create test dataset\n",
    "print(\"\\nCreating test dataset...\")\n",
    "test_dataset = DualStreamRadioMLDataset(\n",
    "    FILE_PATH, JSON_PATH, target_modulations,\n",
    "    mode='test',\n",
    "    indices=splits['test'], \n",
    "    label_map=label_map,\n",
    "    normalization_stats=train_norm_stats,\n",
    "    seed=NORM_SEED\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ All datasets created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8bdc823d-02ad-492a-9175-d7282f95bafd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STEP 4: CREATING DATA LOADERS\n",
      "============================================================\n",
      "‚úÖ Data loaders created successfully\n"
     ]
    }
   ],
   "source": [
    "# ============= Create Data Loaders =============\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 4: CREATING DATA LOADERS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Note: For memory efficiency, consider using smaller num_workers\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True, \n",
    "    pin_memory=torch.cuda.is_available(),  # Only if using GPU\n",
    "    num_workers=min(NUM_WORKERS, 2),  # Reduce for memory efficiency\n",
    "    persistent_workers=False  # Set to False for memory efficiency\n",
    ")\n",
    "\n",
    "valid_loader = DataLoader(\n",
    "    valid_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=False, \n",
    "    pin_memory=torch.cuda.is_available(),\n",
    "    num_workers=min(NUM_WORKERS, 2),\n",
    "    persistent_workers=False\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=False, \n",
    "    pin_memory=torch.cuda.is_available(),\n",
    "    num_workers=min(NUM_WORKERS, 2),\n",
    "    persistent_workers=False\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Data loaders created successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5eba21a3-be12-47f5-be93-4d362b8c49e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      "Layer (type:depth-idx)                   Param #\n",
      "=================================================================\n",
      "MultiDomainFusionModel                   --\n",
      "‚îú‚îÄCnn2DBranch: 1-1                       --\n",
      "‚îÇ    ‚îî‚îÄSequential: 2-1                   --\n",
      "‚îÇ    ‚îÇ    ‚îî‚îÄConv2d: 3-1                  640\n",
      "‚îÇ    ‚îÇ    ‚îî‚îÄBatchNorm2d: 3-2             128\n",
      "‚îÇ    ‚îÇ    ‚îî‚îÄLeakyReLU: 3-3               --\n",
      "‚îÇ    ‚îÇ    ‚îî‚îÄConv2d: 3-4                  73,856\n",
      "‚îÇ    ‚îÇ    ‚îî‚îÄBatchNorm2d: 3-5             256\n",
      "‚îÇ    ‚îÇ    ‚îî‚îÄLeakyReLU: 3-6               --\n",
      "‚îÇ    ‚îÇ    ‚îî‚îÄMaxPool2d: 3-7               --\n",
      "‚îÇ    ‚îÇ    ‚îî‚îÄDropout2d: 3-8               --\n",
      "‚îÇ    ‚îÇ    ‚îî‚îÄConv2d: 3-9                  295,168\n",
      "‚îÇ    ‚îÇ    ‚îî‚îÄBatchNorm2d: 3-10            512\n",
      "‚îÇ    ‚îÇ    ‚îî‚îÄLeakyReLU: 3-11              --\n",
      "‚îÇ    ‚îÇ    ‚îî‚îÄMaxPool2d: 3-12              --\n",
      "‚îÇ    ‚îÇ    ‚îî‚îÄDropout2d: 3-13              --\n",
      "‚îÇ    ‚îî‚îÄAdaptiveAvgPool2d: 2-2            --\n",
      "‚îú‚îÄCnn2DBranch: 1-2                       --\n",
      "‚îÇ    ‚îî‚îÄSequential: 2-3                   --\n",
      "‚îÇ    ‚îÇ    ‚îî‚îÄConv2d: 3-14                 640\n",
      "‚îÇ    ‚îÇ    ‚îî‚îÄBatchNorm2d: 3-15            128\n",
      "‚îÇ    ‚îÇ    ‚îî‚îÄLeakyReLU: 3-16              --\n",
      "‚îÇ    ‚îÇ    ‚îî‚îÄConv2d: 3-17                 73,856\n",
      "‚îÇ    ‚îÇ    ‚îî‚îÄBatchNorm2d: 3-18            256\n",
      "‚îÇ    ‚îÇ    ‚îî‚îÄLeakyReLU: 3-19              --\n",
      "‚îÇ    ‚îÇ    ‚îî‚îÄMaxPool2d: 3-20              --\n",
      "‚îÇ    ‚îÇ    ‚îî‚îÄDropout2d: 3-21              --\n",
      "‚îÇ    ‚îÇ    ‚îî‚îÄConv2d: 3-22                 295,168\n",
      "‚îÇ    ‚îÇ    ‚îî‚îÄBatchNorm2d: 3-23            512\n",
      "‚îÇ    ‚îÇ    ‚îî‚îÄLeakyReLU: 3-24              --\n",
      "‚îÇ    ‚îÇ    ‚îî‚îÄMaxPool2d: 3-25              --\n",
      "‚îÇ    ‚îÇ    ‚îî‚îÄDropout2d: 3-26              --\n",
      "‚îÇ    ‚îî‚îÄAdaptiveAvgPool2d: 2-4            --\n",
      "‚îú‚îÄLstm1dBranch: 1-3                      --\n",
      "‚îÇ    ‚îî‚îÄSequential: 2-5                   --\n",
      "‚îÇ    ‚îÇ    ‚îî‚îÄConv1d: 3-27                 960\n",
      "‚îÇ    ‚îÇ    ‚îî‚îÄBatchNorm1d: 3-28            128\n",
      "‚îÇ    ‚îÇ    ‚îî‚îÄReLU: 3-29                   --\n",
      "‚îÇ    ‚îÇ    ‚îî‚îÄMaxPool1d: 3-30              --\n",
      "‚îÇ    ‚îÇ    ‚îî‚îÄConv1d: 3-31                 57,472\n",
      "‚îÇ    ‚îÇ    ‚îî‚îÄBatchNorm1d: 3-32            256\n",
      "‚îÇ    ‚îÇ    ‚îî‚îÄReLU: 3-33                   --\n",
      "‚îÇ    ‚îÇ    ‚îî‚îÄMaxPool1d: 3-34              --\n",
      "‚îÇ    ‚îî‚îÄLSTM: 2-6                         2,367,488\n",
      "‚îú‚îÄSequential: 1-4                        --\n",
      "‚îÇ    ‚îî‚îÄLinear: 2-7                       524,800\n",
      "‚îÇ    ‚îî‚îÄReLU: 2-8                         --\n",
      "‚îÇ    ‚îî‚îÄDropout: 2-9                      --\n",
      "‚îÇ    ‚îî‚îÄLinear: 2-10                      131,328\n",
      "‚îÇ    ‚îî‚îÄReLU: 2-11                        --\n",
      "‚îÇ    ‚îî‚îÄDropout: 2-12                     --\n",
      "‚îÇ    ‚îî‚îÄLinear: 2-13                      1,285\n",
      "=================================================================\n",
      "Total params: 3,824,837\n",
      "Trainable params: 3,824,837\n",
      "Non-trainable params: 0\n",
      "=================================================================\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from CNN_LSTM_new import create_multi_domain_model\n",
    "    from torchinfo import summary\n",
    "    model_new = create_multi_domain_model(num_classes=NUM_CLASSES, dropout_rate=0.7).to(device)\n",
    "    print(summary(model_new))\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Could not build Multi domain Model: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "16857e21-50b5-48e4-ba6f-d4bd436d0bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizers for both models\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "optimizer_new = optim.AdamW(\n",
    "    model_new.parameters(), \n",
    "    lr=1e-3,\n",
    "    weight_decay=1e-4,\n",
    "    betas=(0.9, 0.99)\n",
    ")\n",
    "\n",
    "# Schedulers for both models\n",
    "scheduler_new = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer_new, \n",
    "    mode='max',           # Monitor accuracy\n",
    "    patience=5,           # Wait for 5 epochs without improvement\n",
    "    factor=0.5,           # Reduce LR by half\n",
    "    min_lr=1e-6,          # Minimum learning rate\n",
    "    #verbose=True          # Print updates\n",
    ")\n",
    "\n",
    "# Scalers for mixed precision\n",
    "scaler_new = torch.cuda.amp.GradScaler()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9d89d448-71ad-49d0-90b2-e34ef24ba6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = {\n",
    "    'train_losses': [], 'valid_losses': [],\n",
    "    'train_accuracies': [], 'valid_accuracies': [],\n",
    "    'training_times': [], 'best_accuracy': 0.0,\n",
    "    'final_predictions': [], 'final_true_labels': []\n",
    "}\n",
    "patience_counter = 0\n",
    "best_model_state = None\n",
    "scaler = GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "94832d6e-7988-4e63-8714-1e79dce4bbb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, optimizer, criterion, scaler, device):\n",
    "    \"\"\"\n",
    "    Train one epoch with the new multi-domain model.\n",
    "    Handles three data inputs from the dataloader.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # --- ADJUSTMENT 1: Unpack the new data format ---\n",
    "    # The dataloader now yields 5 items. We need the first 4.\n",
    "    for amp_inputs, phase_inputs, iq_seq_inputs, labels, _ in train_loader:\n",
    "        # --- ADJUSTMENT 2: Move all required tensors to the device ---\n",
    "        amp_inputs = amp_inputs.to(device)\n",
    "        phase_inputs = phase_inputs.to(device)\n",
    "        iq_seq_inputs = iq_seq_inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        with autocast():\n",
    "            # --- ADJUSTMENT 3: Call the model with three inputs ---\n",
    "            outputs = model(amp_inputs, phase_inputs, iq_seq_inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Mixed-precision training steps (unchanged)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        # Metrics calculation (unchanged)\n",
    "        running_loss += loss.item() * amp_inputs.size(0)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    epoch_time = time.time() - start_time\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    epoch_accuracy = 100. * correct / total\n",
    "    \n",
    "    return epoch_loss, epoch_accuracy, epoch_time\n",
    "\n",
    "def validate_epoch(model, valid_loader, criterion, device):\n",
    "    \"\"\"\n",
    "    Validate one epoch with the new multi-domain model.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # --- ADJUSTMENT 1: Unpack the new data format ---\n",
    "        for amp_inputs, phase_inputs, iq_seq_inputs, labels, _ in valid_loader:\n",
    "            # --- ADJUSTMENT 2: Move all required tensors to the device ---\n",
    "            amp_inputs = amp_inputs.to(device)\n",
    "            phase_inputs = phase_inputs.to(device)\n",
    "            iq_seq_inputs = iq_seq_inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # --- ADJUSTMENT 3: Call the model with three inputs ---\n",
    "            outputs = model(amp_inputs, phase_inputs, iq_seq_inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Metrics calculation (unchanged)\n",
    "            running_loss += loss.item() * amp_inputs.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            predictions.extend(predicted.cpu().numpy())\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    epoch_loss = running_loss / len(valid_loader.dataset)\n",
    "    epoch_accuracy = 100. * correct / total\n",
    "    \n",
    "    return epoch_loss, epoch_accuracy, predictions, true_labels\n",
    "\n",
    "def test_epoch(model, test_loader, criterion, device):\n",
    "    \"\"\"\n",
    "    Test one epoch with the new multi-domain model and a progress bar.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "    \n",
    "    test_iterator = tqdm(test_loader, desc=\"Testing\", leave=False, ncols=100)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # --- ADJUSTMENT 1: Unpack the new data format ---\n",
    "        for amp_inputs, phase_inputs, iq_seq_inputs, labels, _ in test_iterator:\n",
    "            # --- ADJUSTMENT 2: Move all required tensors to the device ---\n",
    "            amp_inputs = amp_inputs.to(device)\n",
    "            phase_inputs = phase_inputs.to(device)\n",
    "            iq_seq_inputs = iq_seq_inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # --- ADJUSTMENT 3: Call the model with three inputs ---\n",
    "            outputs = model(amp_inputs, phase_inputs, iq_seq_inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Metrics calculation (unchanged)\n",
    "            running_loss += loss.item() * amp_inputs.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            predictions.extend(predicted.cpu().numpy())\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "            \n",
    "            # Update progress bar\n",
    "            avg_loss = running_loss / total\n",
    "            accuracy = 100. * correct / total\n",
    "            test_iterator.set_postfix(loss=f\"{avg_loss:.4f}\", accuracy=f\"{accuracy:.2f}%\")\n",
    "\n",
    "    epoch_loss = running_loss / len(test_loader.dataset)\n",
    "    epoch_accuracy = 100. * correct / total\n",
    "\n",
    "    return epoch_loss, epoch_accuracy, predictions, true_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5585be4-9ec8-4537-b9dc-b97b54cfea98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üéØ Starting model training...\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   5%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                                                                                                 | 5/100 [11:22<3:38:19, 137.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5/100:\n",
      "Train Acc: 62.42% | Valid Acc: 63.79% | Time: 80.67s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   7%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                                                                                              | 7/100 [16:04<3:36:18, 139.55s/it]"
     ]
    }
   ],
   "source": [
    "# --- Simplified Training Loop ---\n",
    "import copy\n",
    "import time\n",
    "print(\"\\nüéØ Starting model training...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for epoch in tqdm(range(NUM_EPOCHS), desc=\"Training Progress\"):\n",
    "    epoch_start = time.time()\n",
    "    \n",
    "    # --- Train the model ---\n",
    "    # The train_epoch function is the adjusted one that handles 3 data inputs\n",
    "    train_loss, train_acc, train_time = train_epoch(\n",
    "        model_new, train_loader, optimizer_new, criterion, scaler_new, device\n",
    "    )\n",
    "    \n",
    "    # --- Validate the model ---\n",
    "    # The validate_epoch function is also the adjusted one\n",
    "    valid_loss, valid_acc, predictions, true_labels = validate_epoch(\n",
    "        model_new, valid_loader, criterion, device\n",
    "    )\n",
    "    \n",
    "    # --- Update the flattened metrics dictionary ---\n",
    "    metrics['train_losses'].append(train_loss)\n",
    "    metrics['train_accuracies'].append(train_acc)\n",
    "    metrics['valid_losses'].append(valid_loss)\n",
    "    metrics['valid_accuracies'].append(valid_acc)\n",
    "    metrics['training_times'].append(train_time)\n",
    "    \n",
    "    # --- Update scheduler (if you have one) ---\n",
    "    # scheduler_parallel.step()\n",
    "    \n",
    "    # --- Check for best model and handle early stopping ---\n",
    "    if valid_acc > metrics['best_accuracy']:\n",
    "        metrics['best_accuracy'] = valid_acc\n",
    "        # The validation function already returns numpy arrays\n",
    "        metrics['final_predictions'] = predictions\n",
    "        metrics['final_true_labels'] = true_labels\n",
    "        # Save the best model's state\n",
    "        best_model_state = copy.deepcopy(model_new.state_dict())\n",
    "        patience_counter = 0 # Reset patience\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "    \n",
    "    # --- Print epoch results ---\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print(f\"\\nEpoch {epoch+1}/{NUM_EPOCHS}:\")\n",
    "        print(f\"Train Acc: {train_acc:.2f}% | Valid Acc: {valid_acc:.2f}% | Time: {train_time:.2f}s\")\n",
    "    \n",
    "    # --- Early stopping check ---\n",
    "    if patience_counter >= patience:\n",
    "        print(f\"\\nEarly stopping at epoch {epoch+1} - Model has not improved for {patience} epochs.\")\n",
    "        break\n",
    "\n",
    "print(\"\\nüéâ Training Complete!\")\n",
    "print(f\"Best Validation Accuracy: {metrics['best_accuracy']:.2f}%\")\n",
    "\n",
    "# --- Final Testing on Test Set ---\n",
    "# Check if a test_loader and a saved best model exist\n",
    "if 'test_loader' in locals() and best_model_state is not None:\n",
    "    print(\"\\nüîç Final testing on the test set with the best model...\")\n",
    "    \n",
    "    # Load the best model's weights\n",
    "    model_new.load_state_dict(best_model_state)\n",
    "    \n",
    "    # Test the model\n",
    "    test_loss, test_acc, test_predictions, test_true_labels = test_epoch(\n",
    "        model_new, test_loader, criterion, device\n",
    "    )\n",
    "    \n",
    "    # Store the final test results\n",
    "    metrics['test_accuracy'] = test_acc\n",
    "    metrics['test_loss'] = test_loss\n",
    "    metrics['test_predictions'] = test_predictions\n",
    "    metrics['test_true_labels'] = test_true_labels\n",
    "    \n",
    "    print(f\"Final Test Accuracy: {test_acc:.2f}%\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è No test_loader found or model did not improve - skipping final testing.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f64c27-f240-4962-bf50-eae2d52e9b39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016faf2d-b414-4ec9-96f3-a534fdba84c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TesisNich",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
